import asyncio
import logging
import time
from abc import ABC, abstractmethod
from asyncio import CancelledError, Queue, Task, create_task
from concurrent.futures import ThreadPoolExecutor
from contextlib import suppress
from dataclasses import dataclass
from typing import Final, TypeAlias

from pydantic import (
    ByteSize,
    NonNegativeFloat,
    NonNegativeInt,
    PositiveFloat,
    PositiveInt,
    parse_obj_as,
)
from servicelib.logging_utils import log_context
from watchdog.observers.api import DEFAULT_OBSERVER_TIMEOUT

from ._directory_utils import get_directory_total_size
from ._manager import OutputsManager

PortEvent: TypeAlias = str | None

logger = logging.getLogger(__name__)


_1_MB: Final[PositiveInt] = parse_obj_as(ByteSize, "1mib")
_500_MB: Final[PositiveInt] = parse_obj_as(ByteSize, "500mib")


class BaseDelayPolicy(ABC):
    def get_min_interval(self) -> NonNegativeFloat:  # pylint:disable=no-self-use
        return DEFAULT_OBSERVER_TIMEOUT

    @abstractmethod
    def get_wait_interval(self, dir_size: NonNegativeInt) -> NonNegativeFloat:
        """interval to wait based on directory size"""


class DefaultDelayPolicy(BaseDelayPolicy):
    MIN_WAIT: Final[PositiveFloat] = 1
    MAX_WAIT: Final[PositiveFloat] = 10

    _NORMALIZED_MAX_WAIT: Final[PositiveFloat] = MAX_WAIT - MIN_WAIT

    def get_wait_interval(self, dir_size: NonNegativeInt) -> NonNegativeFloat:
        """
        - 1 second, if size of directory <= 1 megabyte
        - 10 seconds, size of directory > 500 megabytes
        - scales between 1 and 10 linearly with directory size
        """
        if dir_size <= _1_MB:
            return self.MIN_WAIT
        if dir_size >= _500_MB:
            return self.MAX_WAIT

        return self.MIN_WAIT + self._NORMALIZED_MAX_WAIT * dir_size / _500_MB


@dataclass
class TrackedEvent:
    last_detection: NonNegativeFloat
    wait_interval: NonNegativeFloat | None = None


class EventFilter:
    def __init__(
        self,
        outputs_manager: OutputsManager,
        delay_policy: BaseDelayPolicy = DefaultDelayPolicy(),
    ):
        self.outputs_manager = outputs_manager
        self.delay_policy = delay_policy

        self._incoming_events_queue: Queue[PortEvent] = Queue()
        self._task_incoming_event_ingestion: Task | None = None

        self._task_event_emitter: Task | None = None
        self._keep_event_emitter_running: bool = True

        self._upload_events_queue: Queue[str | None] = Queue()
        self._task_upload_events: Task | None = None

        self._port_key_tracked_event: dict[str, TrackedEvent] = {}

    async def _worker_incoming_event_ingestion(self) -> None:
        """processes incoming events generated by the watchdog"""
        while True:
            port_event: PortEvent = await self._incoming_events_queue.get()
            if port_event is None:
                break

            port_key = port_event

            if port_key not in self._port_key_tracked_event:
                self._port_key_tracked_event[port_key] = TrackedEvent(
                    last_detection=time.time()
                )
            else:
                self._port_key_tracked_event[port_key].last_detection = time.time()

    def _worker_blocking_event_emitter(self) -> None:  # NOSONAR
        repeat_interval = self.delay_policy.get_min_interval() * 0.49
        while self._keep_event_emitter_running:
            # can be iterated while modified
            for port_key in list(self._port_key_tracked_event.keys()):
                tracked_event = self._port_key_tracked_event.get(port_key, None)
                if tracked_event is None:  # can disappear while iterated
                    continue

                current_time = time.time()
                elapsed_since_detection = current_time - tracked_event.last_detection

                # ensure minimum interval has passed
                if elapsed_since_detection < (
                    tracked_event.wait_interval or self.delay_policy.get_min_interval()
                ):
                    continue

                # Set the wait_interval for future events.
                # NOTE: Computing the size of a directory is a relatively difficult task,
                # example: on SSD with 1 million files ~ 2 seconds
                # Size of directory will only be computed if:
                # - event was just added
                # - already waited more than the wait_interval
                if (
                    tracked_event.wait_interval is None
                    or elapsed_since_detection > tracked_event.wait_interval
                ):
                    port_key_dir_path = (
                        self.outputs_manager.outputs_context.outputs_path / port_key
                    )
                    total_wait_for = self.delay_policy.get_wait_interval(
                        get_directory_total_size(port_key_dir_path)
                    )
                    tracked_event.wait_interval = total_wait_for

                # could require to wait more since wait_interval was just updated
                elapsed_since_detection = current_time - tracked_event.last_detection
                if elapsed_since_detection < tracked_event.wait_interval:
                    continue

                # event chain has finished, request event to upload port and
                # remove event chain from tracking
                self._upload_events_queue.put_nowait(port_key)
                self._port_key_tracked_event.pop(port_key, None)

            time.sleep(repeat_interval)

    async def _worker_event_emitter(self) -> None:
        """checks at fixed intervals if it should emit events to upload"""
        with ThreadPoolExecutor(max_workers=1) as pool:
            await asyncio.get_event_loop().run_in_executor(
                pool, self._worker_blocking_event_emitter
            )

    async def _worker_upload_events(self) -> None:
        """enqueues uploads for port  `port_key`"""
        while True:
            port_key: str | None = await self._upload_events_queue.get()
            if port_key is None:
                break

            logger.debug("Request upload for port_key %s", port_key)
            await self.outputs_manager.port_key_content_changed(port_key)

    async def enqueue(self, port_key: str) -> None:
        await self._incoming_events_queue.put(port_key)

    async def start(self) -> None:
        with log_context(logger, logging.INFO, f"{EventFilter.__name__} start"):
            self._task_incoming_event_ingestion = create_task(
                self._worker_incoming_event_ingestion(),
                name=self._worker_incoming_event_ingestion.__name__,
            )

            self._keep_event_emitter_running = True
            self._task_event_emitter = create_task(
                self._worker_event_emitter(), name=self._worker_event_emitter.__name__
            )

            self._task_upload_events = create_task(
                self._worker_upload_events(), name=self._worker_upload_events.__name__
            )

    async def shutdown(self) -> None:
        async def _cancel_task(task: Task | None) -> None:
            if task is None:
                return

            task.cancel()
            with suppress(CancelledError):
                await task

        with log_context(logger, logging.INFO, f"{EventFilter.__name__} shutdown"):
            await self._incoming_events_queue.put(None)
            await _cancel_task(self._task_incoming_event_ingestion)

            self._keep_event_emitter_running = False
            await _cancel_task(self._task_event_emitter)

            await self._upload_events_queue.put(None)
            await _cancel_task(self._task_upload_events)
