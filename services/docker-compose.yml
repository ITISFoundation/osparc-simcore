version: "3.8"
services:
  api-server:
    image: ${DOCKER_REGISTRY:-itisfoundation}/api-server:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - WEBSERVER_HOST=${WEBSERVER_HOST:-webserver}
      - LOG_LEVEL=${LOG_LEVEL:-WARNING}
    env_file:
      - ../.env
    deploy:
      labels:
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        # gzip compression
        - traefik.http.middlewares.${SWARM_STACK_NAME}_gzip.compress=true
        # ssl header necessary so that socket.io upgrades correctly from polling to websocket mode. the middleware must be attached to the right connection.
        - traefik.enable=true
        - traefik.http.services.${SWARM_STACK_NAME}_api-server.loadbalancer.server.port=8000
        - traefik.http.services.${SWARM_STACK_NAME}_api-server.loadbalancer.healthcheck.path=/
        - traefik.http.services.${SWARM_STACK_NAME}_api-server.loadbalancer.healthcheck.interval=2000ms
        - traefik.http.services.${SWARM_STACK_NAME}_api-server.loadbalancer.healthcheck.timeout=1000ms
        - traefik.http.routers.${SWARM_STACK_NAME}_api-server.rule=hostregexp(`{host:.+}`) && (Path(`/`, `/v0`) ||  PathPrefix(`/v0/`) || Path(`/api/v0/openapi.json`))
        - traefik.http.routers.${SWARM_STACK_NAME}_api-server.entrypoints=simcore_api
        - traefik.http.routers.${SWARM_STACK_NAME}_api-server.priority=1
        - traefik.http.routers.${SWARM_STACK_NAME}_api-server.middlewares=${SWARM_STACK_NAME}_gzip@docker,ratelimit-${SWARM_STACK_NAME}_api-server
    networks:
      - default

  autoscaling:
    image: ${DOCKER_REGISTRY:-itisfoundation}/autoscaling:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    networks:
      - autoscaling_subnet
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-WARNING}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
          - node.role == manager
      resources:
        reservations:
          cpus: "0.5"
          memory: "512M"
        limits:
          cpus: "0.5"
          memory: "512M"

  catalog:
    image: ${DOCKER_REGISTRY:-itisfoundation}/catalog:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - CATALOG_BACKGROUND_TASK_REST_TIME=${CATALOG_BACKGROUND_TASK_REST_TIME:-60}
      - CATALOG_DEV_FEATURES_ENABLED=${CATALOG_DEV_FEATURES_ENABLED}
      - CATALOG_SERVICES_DEFAULT_RESOURCE=${CATALOG_SERVICES_DEFAULT_RESOURCE}
      - CATALOG_SERVICES_DEFAULT_SPECIFICATIONS=${CATALOG_SERVICES_DEFAULT_SPECIFICATIONS}
      - DIRECTOR_HOST=${DIRECTOR_HOST:-director}
      - DIRECTOR_PORT=${DIRECTOR_PORT:-8080}
      - LOG_LEVEL=${LOG_LEVEL:-WARNING}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_USER=${POSTGRES_USER}
      - TRACING_THRIFT_COMPACT_ENDPOINT=${TRACING_THRIFT_COMPACT_ENDPOINT}
    networks:
      - default

  director:
    image: ${DOCKER_REGISTRY:-itisfoundation}/director:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - DEFAULT_MAX_MEMORY=${DEFAULT_MAX_MEMORY:-0}
      - DEFAULT_MAX_NANO_CPUS=${DEFAULT_MAX_NANO_CPUS:-0}
      - DIRECTOR_REGISTRY_CACHING_TTL=${DIRECTOR_REGISTRY_CACHING_TTL}
      - DIRECTOR_REGISTRY_CACHING=${DIRECTOR_REGISTRY_CACHING}
      - DIRECTOR_SELF_SIGNED_SSL_FILENAME=${DIRECTOR_SELF_SIGNED_SSL_FILENAME}
      - DIRECTOR_SELF_SIGNED_SSL_SECRET_ID=${DIRECTOR_SELF_SIGNED_SSL_SECRET_ID}
      - DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME=${DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME}
      - DIRECTOR_SERVICES_CUSTOM_CONSTRAINTS=${DIRECTOR_SERVICES_CUSTOM_CONSTRAINTS}
      - EXTRA_HOSTS_SUFFIX=${EXTRA_HOSTS_SUFFIX:-undefined}
      - LOGLEVEL=${LOG_LEVEL:-WARNING}
      - MONITORING_ENABLED=${MONITORING_ENABLED:-True}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_ENDPOINT=${POSTGRES_ENDPOINT}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_USER=${POSTGRES_USER}
      - REGISTRY_AUTH=${REGISTRY_AUTH}
      - REGISTRY_PATH=${REGISTRY_PATH}
      - REGISTRY_PW=${REGISTRY_PW}
      - REGISTRY_SSL=${REGISTRY_SSL}
      - REGISTRY_URL=${REGISTRY_URL}
      - REGISTRY_USER=${REGISTRY_USER}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - SIMCORE_SERVICES_NETWORK_NAME=interactive_services_subnet
      - STORAGE_ENDPOINT=${STORAGE_ENDPOINT}
      - SWARM_STACK_NAME=${SWARM_STACK_NAME:-simcore}
      - TRACING_ENABLED=${TRACING_ENABLED:-True}
      - TRACING_ZIPKIN_ENDPOINT=${TRACING_ZIPKIN_ENDPOINT:-http://jaeger:9411}
      - TRAEFIK_SIMCORE_ZONE=${TRAEFIK_SIMCORE_ZONE:-internal_simcore_stack}
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
          - node.role == manager
    networks:
      - default
      - interactive_services_subnet

  director-v2:
    image: ${DOCKER_REGISTRY:-itisfoundation}/director-v2:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - CATALOG_HOST=${CATALOG_HOST:-catalog}
      - CATALOG_PORT=${CATALOG_PORT:-8000}
      - COMPUTATIONAL_BACKEND_DEFAULT_CLUSTER_URL=${COMPUTATIONAL_BACKEND_DEFAULT_CLUSTER_URL:-tcp://dask-scheduler:8786}
      - DIRECTOR_HOST=${DIRECTOR_HOST:-director}
      - DIRECTOR_PORT=${DIRECTOR_PORT:-8080}
      - DIRECTOR_SELF_SIGNED_SSL_FILENAME=${DIRECTOR_SELF_SIGNED_SSL_FILENAME}
      - DIRECTOR_SELF_SIGNED_SSL_SECRET_ID=${DIRECTOR_SELF_SIGNED_SSL_SECRET_ID}
      - DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME=${DIRECTOR_SELF_SIGNED_SSL_SECRET_NAME}
      - DIRECTOR_SERVICES_CUSTOM_CONSTRAINTS=${DIRECTOR_SERVICES_CUSTOM_CONSTRAINTS}
      - DIRECTOR_V2_DEV_FEATURES_ENABLED=${DIRECTOR_V2_DEV_FEATURES_ENABLED}
      - DYNAMIC_SIDECAR_IMAGE=${DOCKER_REGISTRY:-itisfoundation}/dynamic-sidecar:${DOCKER_IMAGE_TAG:-latest}
      - EXTRA_HOSTS_SUFFIX=${EXTRA_HOSTS_SUFFIX:-undefined}
      - LOG_LEVEL=${LOG_LEVEL:-WARNING}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - R_CLONE_PROVIDER=${R_CLONE_PROVIDER}
      - MONITORING_ENABLED=${MONITORING_ENABLED:-True}
      - SIMCORE_SERVICES_NETWORK_NAME=interactive_services_subnet
      - TRACING_THRIFT_COMPACT_ENDPOINT=${TRACING_THRIFT_COMPACT_ENDPOINT}
    env_file:
      - ../.env
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
          - node.role == manager
    networks:
      - default
      - interactive_services_subnet
      - computational_services_subnet

  invitations:
    image: ${DOCKER_REGISTRY:-itisfoundation}/invitations:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    networks:
      - default
    environment:
      - INVITATIONS_LOGLEVEL=${LOG_LEVEL:-INFO}
      - INVITATIONS_SECRET_KEY=${INVITATIONS_SECRET_KEY}
      - INVITATIONS_OSPARC_URL=${INVITATIONS_OSPARC_URL}
      - INVITATIONS_USERNAME=${INVITATIONS_USERNAME}
      - INVITATIONS_PASSWORD=${INVITATIONS_PASSWORD}

  static-webserver:
    image: ${DOCKER_REGISTRY:-itisfoundation}/static-webserver:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8000
      - SERVER_LOG_LEVEL=error
      - SERVER_ROOT=/static-content
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
      labels:
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        - traefik.http.middlewares.${SWARM_STACK_NAME}_gzip.compress=true
        - traefik.enable=true
        - traefik.http.services.${SWARM_STACK_NAME}_static_webserver.loadbalancer.server.port=8000
        - traefik.http.services.${SWARM_STACK_NAME}_static_webserver.loadbalancer.healthcheck.path=/
        - traefik.http.services.${SWARM_STACK_NAME}_static_webserver.loadbalancer.healthcheck.interval=2000ms
        - traefik.http.services.${SWARM_STACK_NAME}_static_webserver.loadbalancer.healthcheck.timeout=1000ms
        - traefik.http.middlewares.${SWARM_STACK_NAME}_static_webserver_retry.retry.attempts=2
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver.rule=hostregexp(`{host:.+}`) && (Path(`/osparc`,`/s4l`,`/s4llite`,`/tis`,`/transpiled`,`/resource`) || PathPrefix(`/osparc/`,`/s4l/`,`/s4llite/`,`/tis/`,`/transpiled/`,`/resource/`))
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver.service=${SWARM_STACK_NAME}_static_webserver
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver.priority=2
        - traefik.http.routers.${SWARM_STACK_NAME}_static_webserver.middlewares=${SWARM_STACK_NAME}_gzip@docker,${SWARM_STACK_NAME}_static_webserver_retry
        # catchall for legacy services (this happens if a backend disappears and a frontend tries to reconnect, the right return value is a 503)
        - traefik.http.routers.${SWARM_STACK_NAME}_legacy_services_catchall.service=${SWARM_STACK_NAME}_legacy_services_catchall
        - traefik.http.routers.${SWARM_STACK_NAME}_legacy_services_catchall.priority=1
        - traefik.http.routers.${SWARM_STACK_NAME}_legacy_services_catchall.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_legacy_services_catchall.rule=hostregexp(`{host:.+}`) && (Path(`/x/{node_uuid:\b[0-9a-f]{8}\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\b[0-9a-f]{12}\b}`) || PathPrefix(`/x/{node_uuid:\b[0-9a-f]{8}\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\b[0-9a-f]{12}\b}/`))
        # this tricks traefik into a 502 (bad gateway) since the service does not exist on this port
        - traefik.http.services.${SWARM_STACK_NAME}_legacy_services_catchall.loadbalancer.server.port=0
        # this tricks traefik into returning a 503 (service unavailable) since the healthcheck will always return false
        - traefik.http.services.${SWARM_STACK_NAME}_legacy_services_catchall.loadbalancer.healthcheck.path=/some/invalid/path/to/generate/a/503
        - traefik.http.services.${SWARM_STACK_NAME}_legacy_services_catchall.loadbalancer.healthcheck.interval=500s
        - traefik.http.services.${SWARM_STACK_NAME}_legacy_services_catchall.loadbalancer.healthcheck.timeout=1ms
        # see [#2718](https://github.com/ITISFoundation/osparc-simcore/issues/2718)
        # catchall for dy-sidecar powered-services (this happens if a backend disappears and a frontend tries to reconnect, the right return value is a 503)
        - traefik.http.routers.${SWARM_STACK_NAME}_modern_services_catchall.service=${SWARM_STACK_NAME}_modern_services_catchall
        # the priority is a bit higher than webserver, the webserver is the fallback to everything and has prio 2
        - traefik.http.routers.${SWARM_STACK_NAME}_modern_services_catchall.priority=3
        - traefik.http.routers.${SWARM_STACK_NAME}_modern_services_catchall.entrypoints=http
        # in theory the pattern should be uuid.services.OSPARC_DOMAIN, but anything could go through.. so let's catch everything
        - traefik.http.routers.${SWARM_STACK_NAME}_modern_services_catchall.rule=hostregexp(`{node_uuid:.+}.services.{host:.+}`)
        # this tricks traefik into a 502 (bad gateway) since the service does not exist on this port
        - traefik.http.services.${SWARM_STACK_NAME}_modern_services_catchall.loadbalancer.server.port=0
        # this tricks traefik into returning a 503 (service unavailable) since the healthcheck will always return false
        - traefik.http.services.${SWARM_STACK_NAME}_modern_services_catchall.loadbalancer.healthcheck.path=/some/invalid/path/to/generate/a/503
        - traefik.http.services.${SWARM_STACK_NAME}_modern_services_catchall.loadbalancer.healthcheck.interval=500s
        - traefik.http.services.${SWARM_STACK_NAME}_modern_services_catchall.loadbalancer.healthcheck.timeout=1ms
    networks:
      - default

  webserver:
    image: ${DOCKER_REGISTRY:-itisfoundation}/webserver:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      &webserver-environment
      CATALOG_HOST: ${CATALOG_HOST:-catalog}
      CATALOG_PORT: ${CATALOG_PORT:-8000}
      DIAGNOSTICS_MAX_AVG_LATENCY: 10
      DIAGNOSTICS_MAX_TASK_DELAY: 30
      DIRECTOR_HOST: ${DIRECTOR_HOST:-director}
      DIRECTOR_PORT: ${DIRECTOR_PORT:-8080}
      DIRECTOR_V2_HOST: ${DIRECTOR_V2_HOST:-director-v2}
      DIRECTOR_V2_PORT: ${DIRECTOR_V2_PORT:-8000}
      STORAGE_HOST: ${STORAGE_HOST:-storage}
      STORAGE_PORT: ${STORAGE_PORT:-8080}
      SWARM_STACK_NAME: ${SWARM_STACK_NAME:-simcore}
      WEBSERVER_GARBAGE_COLLECTOR: "null"
      WEBSERVER_LOGLEVEL: ${LOG_LEVEL:-WARNING}
    env_file:
      - ../.env
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
      labels:
        - io.simcore.zone=${TRAEFIK_SIMCORE_ZONE}
        # gzip compression
        - traefik.http.middlewares.${SWARM_STACK_NAME}_gzip.compress=true
        # ssl header necessary so that socket.io upgrades correctly from polling to websocket mode. the middleware must be attached to the right connection.
        - traefik.http.middlewares.${SWARM_STACK_NAME_NO_HYPHEN}_sslheader.headers.customrequestheaders.X-Forwarded-Proto=http
        - traefik.enable=true
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.server.port=8080
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.healthcheck.path=/v0/
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.healthcheck.interval=2000ms
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.healthcheck.timeout=1000ms
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.sticky.cookie=true
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.sticky.cookie.samesite=lax
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.sticky.cookie.httponly=true
        - traefik.http.services.${SWARM_STACK_NAME}_webserver.loadbalancer.sticky.cookie.secure=true
        - traefik.http.middlewares.${SWARM_STACK_NAME}_webserver_retry.retry.attempts=2
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.service=${SWARM_STACK_NAME}_webserver
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.rule=hostregexp(`{host:.+}`) && (Path(`/`, `/v0`,`/socket.io/`,`/static-frontend-data.json`, `/study/{study_uuid:\b[0-9a-f]{8}\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\b[0-9a-f]{12}\b}`, `/view`, `/#/view`, `/#/error`) ||  PathPrefix(`/v0/`))
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.entrypoints=http
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.priority=2
        - traefik.http.routers.${SWARM_STACK_NAME}_webserver.middlewares=${SWARM_STACK_NAME}_gzip@docker, ${SWARM_STACK_NAME_NO_HYPHEN}_sslheader@docker, ${SWARM_STACK_NAME}_webserver_retry
    networks:
      - default
      - interactive_services_subnet

  wb-garbage-collector:
    image: ${DOCKER_REGISTRY:-itisfoundation}/webserver:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      <<: *webserver-environment
      WEBSERVER_GARBAGE_COLLECTOR: '{"GARBAGE_COLLECTOR_INTERVAL_S": 30}'
    env_file:
      - ../.env
      - ../.env-wb-garbage-collector

    deploy:
      placement:
        constraints:
          - node.platform.os == linux
    networks:
      - default
      - interactive_services_subnet

  agent:
    image: ${DOCKER_REGISTRY:-itisfoundation}/agent:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}"
    deploy:
      mode: global
      resources:
        limits:
          cpus: "1.0"
          memory: 1024M
    networks:
      - default
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      LOGLEVEL: ${LOG_LEVEL:-INFO}
      AGENT_VOLUMES_CLEANUP_S3_SECURE: ${AGENT_VOLUMES_CLEANUP_S3_SECURE}
      AGENT_VOLUMES_CLEANUP_S3_ENDPOINT: ${AGENT_VOLUMES_CLEANUP_S3_ENDPOINT}
      AGENT_VOLUMES_CLEANUP_S3_ACCESS_KEY: ${AGENT_VOLUMES_CLEANUP_S3_ACCESS_KEY}
      AGENT_VOLUMES_CLEANUP_S3_SECRET_KEY: ${AGENT_VOLUMES_CLEANUP_S3_SECRET_KEY}
      AGENT_VOLUMES_CLEANUP_S3_BUCKET: ${AGENT_VOLUMES_CLEANUP_S3_BUCKET}
      AGENT_VOLUMES_CLEANUP_S3_PROVIDER: ${AGENT_VOLUMES_CLEANUP_S3_PROVIDER}
      AGENT_VOLUMES_CLEANUP_S3_REGION: ${AGENT_VOLUMES_CLEANUP_S3_REGION:-us-east-1}

  dask-sidecar:
    image: ${DOCKER_REGISTRY:-itisfoundation}/dask-sidecar:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}"
    deploy:
      mode: global
      endpoint_mode: dnsrr
      resources:
        reservations:
          cpus: "0.10"
          memory: "100M"
    volumes:
      - computational_shared_data:${SIDECAR_COMP_SERVICES_SHARED_FOLDER:-/home/scu/computational_shared_data}
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      &sidecar-environment
      DASK_SCHEDULER_HOST: ${DASK_SCHEDULER_HOST:-dask-scheduler}
      SIDECAR_LOGLEVEL: ${LOG_LEVEL:-WARNING}
      SIDECAR_COMP_SERVICES_SHARED_VOLUME_NAME: ${SWARM_STACK_NAME}_computational_shared_data
      SIDECAR_COMP_SERVICES_SHARED_FOLDER: ${SIDECAR_COMP_SERVICES_SHARED_FOLDER:-/home/scu/computational_shared_data}
    networks:
      - computational_services_subnet

  dask-scheduler:
    image: ${DOCKER_REGISTRY:-itisfoundation}/dask-sidecar:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      <<: *sidecar-environment
      DASK_START_AS_SCHEDULER: 1

    networks:
      - computational_services_subnet

  datcore-adapter:
    image: ${DOCKER_REGISTRY:-itisfoundation}/datcore-adapter:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    networks:
      - storage_subnet
    environment:
      - TRACING_THRIFT_COMPACT_ENDPOINT=${TRACING_THRIFT_COMPACT_ENDPOINT}

  storage:
    image: ${DOCKER_REGISTRY:-itisfoundation}/storage:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - BF_API_KEY=${BF_API_KEY}
      - BF_API_SECRET=${BF_API_SECRET}
      - DATCORE_ADAPTER_HOST=${DATCORE_ADAPTER_HOST:-datcore-adapter}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_ENDPOINT=${POSTGRES_ENDPOINT}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_USER=${POSTGRES_USER}
      - S3_ACCESS_KEY=${S3_ACCESS_KEY}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ENDPOINT=${S3_ENDPOINT}
      - S3_SECRET_KEY=${S3_SECRET_KEY}
      - S3_SECURE=${S3_SECURE}
      - STORAGE_LOGLEVEL=${LOG_LEVEL:-WARNING}
      - STORAGE_MONITORING_ENABLED=1
      - TRACING_ZIPKIN_ENDPOINT=${TRACING_ZIPKIN_ENDPOINT:-http://jaeger:9411}
    deploy:
      placement:
        constraints:
          - node.platform.os == linux
    networks:
      - default
      - interactive_services_subnet
      - storage_subnet

  rabbit:
    image: itisfoundation/rabbitmq:3.11.2-management
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBIT_USER}
      - RABBITMQ_DEFAULT_PASS=${RABBIT_PASSWORD}
    networks:
      - default
      - interactive_services_subnet
      - autoscaling_subnet
    healthcheck:
      # see https://www.rabbitmq.com/monitoring.html#individual-checks for info about health-checks available in rabbitmq
      test: rabbitmq-diagnostics -q status
      interval: 5s
      timeout: 30s
      retries: 5
      start_period: 5s

  migration:
    image: ${DOCKER_REGISTRY:-itisfoundation}/migration:${DOCKER_IMAGE_TAG:-latest}
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_ENDPOINT=${POSTGRES_ENDPOINT}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_USER=${POSTGRES_USER}
    networks:
      - default # actually needed for the postgres service only

  postgres:
    image: "postgres:14.5-alpine@sha256:db802f226b620fc0b8adbeca7859eb203c8d3c9ce5d84870fadee05dea8f50ce"
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - type: tmpfs
        target: /dev/shm
        tmpfs:
          size: 16000000000
    networks:
      - default
      - interactive_services_subnet
    healthcheck:
      test:
        [
          "CMD",
          "pg_isready",
          "--username",
          "${POSTGRES_USER}",
          "--dbname",
          "${POSTGRES_DB}"
        ]
      interval: 5s
      retries: 5
    # NOTES: this is not yet compatible with portainer deployment but could work also for other containers
    # works with Docker 19.03 and not yet with Portainer 1.23.0 (see https://github.com/portainer/portainer/issues/3551)
    # in the meantime postgres allows to set a configuration through CLI.
    # sysctls:
    #   # NOTES: these values are needed here because docker swarm kills long running idle
    #   # connections by default after 15 minutes see https://github.com/moby/moby/issues/31208
    #   # info about these values are here https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html
    #   - net.ipv4.tcp_keepalive_intvl=600
    #   - net.ipv4.tcp_keepalive_probes=9
    #   - net.ipv4.tcp_keepalive_time=600
    command:
      [
        "postgres",
        "-c",
        "tcp_keepalives_idle=600",
        "-c",
        "tcp_keepalives_interval=600",
        "-c",
        "tcp_keepalives_count=5",
        "-c",
        "max_connections=413",
        "-c",
        "shared_buffers=256MB"
      ]

  redis:
    image: "redis:6.2.6@sha256:4bed291aa5efb9f0d77b76ff7d4ab71eee410962965d052552db1fb80576431d"
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    command:
      # redis server will write a backup every 60 seconds if at least 1 key was changed
      # also aof (append only) is also enabled such that we get full durability at the expense
      # of backup size. The backup is written into /data.
      # https://redis.io/topics/persistence
      [
        "redis-server",
        "--save",
        "60 1",
        "--loglevel",
        "verbose",
        "--databases",
        "5",
        "--appendonly",
        "yes"
      ]
    networks:
      - default
      - autoscaling_subnet
    volumes:
      - redis-data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 30s
      retries: 50

  traefik:
    image: "traefik:v2.9.8@sha256:553239e27c4614d0477651415205b9b119f7a98f698e6562ef383c9d8ff3b6e6"
    init: true
    hostname: "{{.Node.Hostname}}-{{.Service.Name}}-{{.Task.Slot}}"
    command:
      - "--api=true"
      - "--api.dashboard=true"
      - "--ping=true"
      - "--entryPoints.ping.address=:9082"
      - "--ping.entryPoint=ping"
      - "--log.level=WARNING"
      - "--accesslog=false"
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
      - "--metrics.prometheus.addServicesLabels=true"
      - "--entryPoints.metrics.address=:8082"
      - "--metrics.prometheus.entryPoint=metrics"
      - "--entryPoints.http.address=:80"
      - "--entryPoints.http.forwardedHeaders.insecure"
      - "--entryPoints.simcore_api.address=:10081"
      - "--entryPoints.simcore_api.forwardedHeaders.insecure"
      - "--entryPoints.traefik_monitor.address=:8080"
      - "--entryPoints.traefik_monitor.forwardedHeaders.insecure"
      - "--providers.docker.endpoint=unix:///var/run/docker.sock"
      - "--providers.docker.network=${SWARM_STACK_NAME}_default"
      - "--providers.docker.swarmMode=true"
      # https://github.com/traefik/traefik/issues/7886
      - "--providers.docker.swarmModeRefreshSeconds=1"
      - "--providers.docker.exposedByDefault=false"
      - "--providers.docker.constraints=Label(`io.simcore.zone`, `${TRAEFIK_SIMCORE_ZONE}`)"
      - "--tracing=true"
      - "--tracing.jaeger=true"
      - "--tracing.jaeger.samplingServerURL=http://jaeger:5778/sampling"
      - "--tracing.jaeger.localAgentHostPort=jaeger:6831"
    volumes:
      # So that Traefik can listen to the Docker events
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      placement:
        constraints:
          - node.role == manager
      labels:
        # for each service in the stack a new middlaware for rate limiting needs to be registered here
        # requests = average / period this is how the limits are defined
        - traefik.http.middlewares.ratelimit-${SWARM_STACK_NAME}_api-server.ratelimit.average=1
        - traefik.http.middlewares.ratelimit-${SWARM_STACK_NAME}_api-server.ratelimit.period=1m
        # a burst is computed over a period of 1 second
        - traefik.http.middlewares.ratelimit-${SWARM_STACK_NAME}_api-server.ratelimit.burst=10
        # X-Forwarded-For header extracts second IP from the right, count starts at one
        - traefik.http.middlewares.ratelimit-${SWARM_STACK_NAME}_api-server.ratelimit.sourcecriterion.ipstrategy.depth=2
    networks:
      - default
      - interactive_services_subnet # for legacy dynamic services
    #healthcheck:
    #  test: wget --quiet --tries=1 --spider http://localhost:9082/ping || exit 1
    #  interval: 3s
    #  timeout: 1s
    #  retries: 3
    #  start_period: 20s


volumes:
  postgres_data:
    name: ${SWARM_STACK_NAME}_postgres_data
  computational_shared_data:
    name: ${SWARM_STACK_NAME}_computational_shared_data
  redis-data:
    name: ${SWARM_STACK_NAME}_redis-data

networks:
  default:
    attachable: true
    name: ${SWARM_STACK_NAME}_default
  storage_subnet:
    attachable: true
    name: ${SWARM_STACK_NAME}_storage_subnet
  autoscaling_subnet:
    attachable: true
    name: ${SWARM_STACK_NAME}_autoscaling_subnet
  interactive_services_subnet:
    name: ${SWARM_STACK_NAME}_interactive_services_subnet
    driver: overlay
    attachable: true
    internal: false
    labels:
      com.simcore.description: "interactive services network"
    ipam:
      driver: default
      config:
        - subnet: "172.8.0.0/16"
  computational_services_subnet:
    name: ${SWARM_STACK_NAME}_computational_services_subnet
    driver: overlay
    attachable: true
    internal: false
    labels:
      com.simcore.description: "computational services network"
